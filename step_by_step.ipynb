{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice on how to build an end to end model given any sort of data\n",
    "\n",
    "Since my interview will fully rely on how I can build a model from start to finish give any set of data, this notebook will act as a guide on how to make that end to end modelin possible. \n",
    "\n",
    "## Steps for end to end model:\n",
    "\n",
    "assuming you have a specific dataset:\n",
    "\n",
    "**Understand the Problem Statement**:\n",
    "\n",
    "Before diving into building a machine learning model, it's crucial to have a clear understanding of the problem you're trying to solve. Define your problem statement, objectives, and success criteria.\n",
    "\n",
    "**Collect and Explore the Data**:\n",
    "\n",
    "Obtain the dataset relevant to your problem statement. Understand the structure of the data, its features, and the target variable.\n",
    "Perform exploratory data analysis (EDA) to gain insights into the data. This includes visualizations, summary statistics, and identifying patterns or correlations in the data.\n",
    "\n",
    "**Data Preprocessing**:\n",
    "\n",
    "Clean the data by handling missing values, outliers, and inconsistencies.\n",
    "Encode categorical variables if necessary (e.g., one-hot encoding, label encoding).\n",
    "Scale or normalize numerical features to ensure they're on similar scales.\n",
    "\n",
    "**Feature Engineering**:\n",
    "\n",
    "Create new features that might be useful for the model.\n",
    "Select relevant features based on domain knowledge or feature importance techniques.\n",
    "\n",
    "**Split the Data**:\n",
    "\n",
    "Split the dataset into training, validation, and test sets. A common split is 70% for training, 15% for validation, and 15% for testing.\n",
    "Ensure the splits maintain the distribution of the target variable, especially in cases of imbalanced datasets.\n",
    "\n",
    "**Select a Model**:\n",
    "\n",
    "Choose a suitable machine learning algorithm based on the nature of your problem (classification, regression, clustering, etc.), the size of your dataset, and computational resources.\n",
    "Consider starting with simple models like linear regression/classification and gradually move to more complex ones like decision trees, random forests, support vector machines, or neural networks.\n",
    "\n",
    "**Train the Model**:\n",
    "\n",
    "Train the selected model on the training dataset using appropriate training techniques (e.g., gradient descent, backpropagation).\n",
    "Tune hyperparameters using techniques like grid search, random search, or Bayesian optimization to improve model performance.\n",
    "\n",
    "**Evaluate the Model**:\n",
    "\n",
    "Assess the model's performance on the validation set using appropriate evaluation metrics (accuracy, precision, recall, F1-score, RMSE, etc.).\n",
    "Adjust the model or experiment with different algorithms if performance is not satisfactory.\n",
    "\n",
    "**Fine-tune the Model**:\n",
    "\n",
    "Refine the model by adjusting parameters or features based on insights gained from the evaluation phase.\n",
    "Avoid overfitting by regularization techniques (e.g., L1/L2 regularization, dropout).\n",
    "\n",
    "**Validate the Model**:\n",
    "\n",
    "Once satisfied with the model's performance on the validation set, evaluate it on the test set to get an unbiased estimate of its performance.\n",
    "\n",
    "**Interpret the Results**:\n",
    "\n",
    "Interpret the model's predictions and understand how it's making decisions.\n",
    "Analyze important features and their impact on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "\n",
    "# feature selection imports\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# feature engineering imports\n",
    "\n",
    "# model imports \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# validation imports\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "# utils\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "1. Start with finding all categorical and non categorical data.\n",
    "\n",
    "```python\n",
    "        non_num_columns = train_data.select_dtypes(exclude=np.number).columns\n",
    "        num_columns = train_data.select_dtypes(np.number).columns\n",
    "```\n",
    "\n",
    "2. Find all nulls in data for categorical and non categorical and draw it\n",
    "\n",
    "\n",
    "```python\n",
    "        null_df = pd.DataFrame(train_data.isnull().sum(), columns=[\"nulls\"]).reset_index()\n",
    "        null_df.rename(columns={\"index\":\"features\"}, inplace=True)\n",
    "\n",
    "        null_df[\"percentage_null\"] = null_df.loc[np.where(null_df.nulls > 0)].nulls/train_data.shape[0]*100\n",
    "        plt.figure(figsize=(10, 25))\n",
    "        sb.catplot(null_df.loc[np.where(null_df.nulls > 0)], x=\"percentage_null\", y=\"features\", kind=\"bar\",height=10)\n",
    "```\n",
    "\n",
    "3. Impute all data with nulls. \n",
    "\n",
    "    1. if `numerical` use `median` can add a bit of noise within same variance range.\n",
    "    2. if `categorical`, introduce `\"unknown\"` as a new category.\n",
    "\n",
    "> For Numerical\n",
    "```python\n",
    "        # Step 1: Identify columns with null values\n",
    "        columns_with_nulls = X.columns[X.isnull().any()].tolist()\n",
    "\n",
    "        # Step 2: Calculate column-wise averages for the columns with nulls\n",
    "        column_averages = X.mean()\n",
    "\n",
    "        # Step 3: Replace null values with column averages\n",
    "        for col in columns_with_nulls:\n",
    "                X[col] = X[col].fillna(column_averages[col])\n",
    "```\n",
    "\n",
    "> For Categorical\n",
    "\n",
    "```python\n",
    "        # Calculate column-wise averages for the columns with nulls\n",
    "        column_averages = \"unknown\"\n",
    "\n",
    "        #  Replace null values with column averages\n",
    "        for col in non_num_columns_new:\n",
    "        X[col] = X[col].fillna(column_averages)\n",
    "```\n",
    "\n",
    "4. **CLEAR DUPLICATES**\n",
    "\n",
    "```python\n",
    "        df.drop_duplicates(subset=[`<column>`])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do minor EDA to look at how the data looks like\n",
    "\n",
    "\n",
    "1. Start off with checking spread target. if categorical `catplot with count`. If numerical `kde density`\n",
    "\n",
    "```python\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    \"\"\"Working with categorical target\"\"\"\n",
    "    sb.catplot(train_data, x=\"target\", kind=\"count\")\n",
    "\n",
    "    \"\"\"Numerical Target\"\"\"\n",
    "    sb.kdeplot(rejoined_xy, x= \"SalePrice\")\n",
    "\n",
    "```\n",
    "\n",
    "2. If a mix of categorical target and numerical features use:\n",
    "\n",
    "```python\n",
    "    fig, axs = plt.subplots(ncols=2,nrows=2, figsize=(25, 25))\n",
    "    \n",
    "    # Numerical\n",
    "    features_lines = [\"BsmtFinSF1\", \"GarageArea\",\"GrLivArea\", \"1stFlrSF\"]\n",
    "    \n",
    "    # overallqual category\n",
    "    col_i = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2): \n",
    "            feature = features_lines[col_i]\n",
    "            sb.lineplot(rejoined_xy, y= feature,x=\"OverallQual\", ax=axs[i,j])\n",
    "            col_i+=1\n",
    "```\n",
    "\n",
    "3. To check overall pattern for num vs num regplot or scatter plot\n",
    "\n",
    "```python\n",
    "    sb.regplot(data=mpg, x=\"displacement\", y=\"mpg\", logx=True)\n",
    "    sb.regplot(x_estimator=np.mean, order=2)\n",
    "```\n",
    "\n",
    "4. Check for data outliers\n",
    "\n",
    "```python\n",
    "    sb.boxplot(rejoined_xy, x= \"SalePrice\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Steps:\n",
    "1. Remove outliers\n",
    "\n",
    "```python\n",
    "    def remove_outlier(df, column_name):\n",
    "        # Calculate the first quartile (Q1) and third quartile (Q3)\n",
    "        column_data = df[column_name]\n",
    "        Q1 = df.quantile(0.25)[column_name]\n",
    "        Q3 = df.quantile(0.75)[column_name]\n",
    "\n",
    "        # Calculate the IQR (Interquartile Range)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define the lower and upper bounds for outlier detection\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        filtered_data = (column_data>=lower_bound) & (column_data <= upper_bound) \n",
    "\n",
    "        # # Filter out rows with outliers\n",
    "        df_no_outliers = df[filtered_data]\n",
    "\n",
    "        return df_no_outliers \n",
    "    \n",
    "```\n",
    "\n",
    "2. OneHotEncode Categorical data\n",
    "\n",
    "```python\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    encodeder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    encoded_data = encodeder.fit_transform(X[cat_columns])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns=encodeder.get_feature_names(cat_columns))\n",
    "\n",
    "    X = pd.concat([X,encoded_df], axis=1) # X with cat dropped\n",
    "```\n",
    "\n",
    "3. Consider binning your data:\n",
    "\n",
    "```python\n",
    "    from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "\n",
    "    binner = KBinsDiscretizer(n_bins=7, encode=\"ordinal\", strategy=\"uniform\")\n",
    "    binned_grlivarea = binner.fit_transform(pd.DataFrame(X.GrLivArea))\n",
    "    X[\"GrLivBinned\"] = binned_grlivarea\n",
    "    X[\"GrLivBinned\"]\n",
    "\n",
    "```\n",
    "4. After feature importance, consider scaling your data.\n",
    "\n",
    "\n",
    "```python\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features_x = scaler.fit_transform(resampled_train_x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature Selection\n",
    "\n",
    "Three main methods for feature selection:\n",
    "\n",
    "```python\n",
    "    ## Form new train data X with the old data that adds encoding\n",
    "    to_corr_df = pd.concat([X,y], axis=1)\n",
    "    corr_df = to_corr_df.corr(method=\"pearson\")\n",
    "\n",
    "    plt.figure(figsize=(10, 25))\n",
    "    sb.heatmap(pd.DataFrame(corr_df[\"SalePrice\"]), cbar=True, cmap=\"Blues\")\n",
    "```\n",
    "\n",
    "```python\n",
    "    info_gain_df = pd.DataFrame(mutual_info_regression(X,y), columns=[\"info_gain\"], index=X.columns).reset_index().rename(columns={'index': 'features'})\n",
    "\n",
    "    plt.figure(figsize=(10, 25))\n",
    "    sb.catplot(info_gain_df, x=\"info_gain\", y=\"features\", kind=\"bar\",height=10)\n",
    "```\n",
    "\n",
    "```python\n",
    "        rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = rf.feature_importances_\n",
    "    features_to_keep = pd.concat([features_to_keep, fold_importance_df], axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split with cross validation\n",
    "\n",
    "\n",
    "> Train test split: \n",
    "\n",
    "```python\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_pca, resampled_train_y, test_size=0.10, shuffle= True, random_state=42)\n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "> cross validate\n",
    "\n",
    "```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error') # or r2\n",
    "\n",
    "    # ROC And ROC_AUC or f1 score for classification\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set model pipeline\n",
    "\n",
    "\n",
    "Show the interviewer that you can set up all the previous stuff as a preprocessor thats extended\n",
    "\n",
    "```python\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "    class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, new_feature_value):\n",
    "            self.new_feature_value = new_feature_value\n",
    "            \n",
    "        def fit(self, X, y=None):\n",
    "            # No fitting necessary, so we return self\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X):\n",
    "            # Add a new feature with the specified value to each sample\n",
    "            new_feature = [self.new_feature_value] * len(X)\n",
    "            return np.c_[X, new_feature]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "1. Regression\n",
    "\n",
    "Start with normal regression\n",
    "\n",
    "if have already selected features --> ridge regression else lasso\n",
    "\n",
    "```python\n",
    "    models = {\n",
    "            (\"Linear Regression\", LinearRegression()),\n",
    "            (\"Ridge Regression\", Ridge()),\n",
    "            (\"Lasso Regression\", Lasso()),\n",
    "            (\"Decision Tree Regression\", DecisionTreeRegressor()),\n",
    "\n",
    "    }\n",
    "\n",
    "    params ={\n",
    "            \"Linear Regression\", LinearRegression()\n",
    "            \"Ridge Regression\", Ridge(),\n",
    "            \"Lasso Regression\", Lasso(),\n",
    "            \"Decision Tree Regression\", DecisionTreeRegressor(),\n",
    "    }\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"{name}: Mean Squared Error = {mse:.2f}\")\n",
    "\n",
    "```\n",
    "2. Classification\n",
    "\n",
    "```python\n",
    "# Create and train various classification models\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"Decision Tree Classifier\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest Classifier\", RandomForestClassifier()),\n",
    "    (\"Gradient Boosting Classifier\", GradientBoostingClassifier()),\n",
    "    (\"Support Vector Classifier\", SVC()),\n",
    "    (\"K-Nearest Neighbors Classifier\", KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    # Fit the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name}: Accuracy = {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "3. Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
